# -1 问题



# 0 规划

- [ ] 如何实现qemu/kvm对Guest MMIO访问的捕获（qemu/kvm切入Guest前的设置）
  - [ ] kvmtool + kvm
- [ ] kvm解析MMIO指令，记录mmio_reason，转发到qemu侧处理（简单分析指令解析流程）
  - [ ] kvm
- [ ] 设备模拟（这里只讨论用户态，但还有些设备，比如中断相关的aplic/imsic在kvm中模拟）
  - [ ] 完全模拟设备：qemu-edu
    - [ ] 中断注入
  - [ ] 共享虚拟设备，虚拟设备前端/后端：网卡模拟
    - [ ] 中断注入

---

- [ ] 设备直通，暂不分析

---

> `kvmtool`：https://github.com/avpatel/kvmtool.git
>
> `kvm-v6.8`：https://github.com/avpatel/linux.git 

# 1 Guest MMIO访问捕获

## 1.1 kvmtool虚机内存配置

以下是kvmtool对虚拟机普通内存的配置流程：

```c
core_init(kvm__init);
	+-> kvm__arch_init 
       	kvm->ram_size = min(kvm->cfg.ram_size, (u64)RISCV_MAX_MEMORY(kvm)); //ram_size
		kvm->arch.ram_alloc_start = mmap_anon_or_hugetlbfs(...); //hva
	+-> kvm__init_ram
        +-> #define RISCV_RAM		0x80000000ULL
        	phys_start	= RISCV_RAM;
        +-> kvm__register_ram(kvm, phys_start, phys_size, host_mem);
			+-> kvm__register_mem(kvm, guest_phys, size, 
                                  		userspace_addr, KVM_MEM_TYPE_RAM);
				
                +-> /* 用户向KVM注册普通内存 */
                    mem = (struct kvm_userspace_memory_region) {
                        .slot			= slot,
                        .flags			= flags,
                        .guest_phys_addr	= guest_phys,
                        .memory_size		= size,
                        .userspace_addr		= (unsigned long)userspace_addr,
                    };
					ret = ioctl(kvm->vm_fd, KVM_SET_USER_MEMORY_REGION, &mem);
```

kvmtool配置虚拟机主存的流程为：

* `kvm__arch_init` 分配虚拟机内存，此时仅为一段hva范围，包括页面大小、起始地址；
* `kvm__init_ram` 为虚拟机注册一段内存空间，这里才是真正的gpa设置，同时也需要传入之前确定的hva区间起始地址；
  * `kvm__register_mem` 将执行具体的guest内存注册流程，内存类型为 `KVM_MEM_TYPE_RAM`；
    * kvmtool需要对guest内存进行管理，各段内存用 `struct kvm_mem_bank` 表示，所有的内存区域组织为一个链表，链表头为 `kvm->mem_banks`（还涉及到重叠内存区域的处理，这里不讨论）；
    * 向KVM提交用户态的虚拟机内存配置，KVM需要的结构为 `kvm_userspace_memory_region`，调用 `ioctl(KVM_SET_USER_MEMORY_REGION)` 陷入KVM；

> 总的来看，至少在 `kvm__init` 的初始化流程中，只对普通内存类型 `KVM_MEM_TYPE_RAM` 进行了分配并向KVM注册。

---

- [x] 以plic为例，`kvmtool-riscv` mmio内存注册流程如下：

```c
dev_init(plic__init);
	+-> kvm__register_mmio(kvm, RISCV_IRQCHIP, RISCV_IRQCHIP_SIZE,
				 false, plic__mmio_callback, &plic);
		+-> kvm__register_iotrap(kvm, phys_addr, phys_addr_len, mmio_fn, ptr,
					DEVICE_BUS_MMIO | (coalesce ? IOTRAP_COALESCE : 0));
	+-> /* Setup default IRQ routing */
        plic__irq_routing_init(kvm);
```

- [x] `kvm__register_mmio` 代码如下：

  ```c
  kvm__register_mmio(kvm, RISCV_IRQCHIP, RISCV_IRQCHIP_SIZE,
  				 false, plic__mmio_callback, &plic);
  
  static inline
  int __must_check kvm__register_mmio(struct kvm *kvm, u64 phys_addr,
  				    u64 phys_addr_len, bool coalesce,
  				    mmio_handler_fn mmio_fn, void *ptr)
  {
  	return kvm__register_iotrap(kvm, phys_addr, phys_addr_len, mmio_fn, ptr,
  			DEVICE_BUS_MMIO | (coalesce ? IOTRAP_COALESCE : 0));
  }
  
  int kvm__register_iotrap(struct kvm *kvm, u64 phys_addr, u64 phys_addr_len,
  			 mmio_handler_fn mmio_fn, void *ptr,
  			 unsigned int flags)
  {
  	struct mmio_mapping *mmio;
  	struct kvm_coalesced_mmio_zone zone;
  	int ret;
  
  	mmio = malloc(sizeof(*mmio));
  	if (mmio == NULL)
  		return -ENOMEM;
  	
      /*
      	phys_addr = RISCV_IRQCHIP
      	phys_addr_len = RISCV_IRQCHIP_SIZE
      	mmio_fn = plic__mmio_callback
      	ptr = &plic
      */
  	*mmio = (struct mmio_mapping) {
  		.node		= RB_INT_INIT(phys_addr, phys_addr + phys_addr_len),
  		.mmio_fn	= mmio_fn,
  		.ptr		= ptr,
  		/*
  		 * Start from 0 because kvm__deregister_mmio() doesn't decrement
  		 * the reference count.
  		 */
  		.refcount	= 0,
  		.remove		= false,
  	};
  
      // flags = DEVICE_BUS_MMIO | IOTRAP_COALESCE
  	if (trap_is_mmio(flags) && (flags & IOTRAP_COALESCE)) {
  		zone = (struct kvm_coalesced_mmio_zone) {
  			.addr	= phys_addr,
  			.size	= phys_addr_len,
  		};
  		ret = ioctl(kvm->vm_fd, KVM_REGISTER_COALESCED_MMIO, &zone);
  		if (ret < 0) {
  			free(mmio);
  			return -errno;
  		}
  	}
  
      //static struct rb_root mmio_tree = RB_ROOT;
  	mutex_lock(&mmio_lock);
  	if (trap_is_mmio(flags))
  		ret = mmio_insert(&mmio_tree, mmio);
  	else
  		ret = mmio_insert(&pio_tree, mmio);
  	mutex_unlock(&mmio_lock);
  
  	return ret;
  }
  ```

  ---

- [x] `plic__irq_routing_init` ，设置默认中断路由：

  ```c
  static int plic__irq_routing_init(struct kvm *kvm)
  {
  	int r;
  
  	/*
  	 * This describes the default routing that the kernel uses without
  	 * any routing explicitly set up via KVM_SET_GSI_ROUTING. So we
  	 * don't need to commit these setting right now. The first actual
  	 * user (MSI routing) will engage these mappings then.
  	 */
  	for (next_gsi = 0; next_gsi < MAX_DEVICES; next_gsi++) {
  		r = irq__allocate_routing_entry();
  		if (r)
  			return r;
  
  		irq_routing->entries[irq_routing->nr++] =
  			(struct kvm_irq_routing_entry) {
  				.gsi = next_gsi,
  				.type = KVM_IRQ_ROUTING_IRQCHIP,
  				.u.irqchip.irqchip = IRQCHIP_PLIC_NR,
  				.u.irqchip.pin = next_gsi,
  		};
  	}
  
  	return 0;
  }
  ```

  内核在没有通过 `KVM_SET_GSI_ROUTING` 明确设置任何路由的情况下使用的默认路由。因此，我们现在不需要提交这些设置。第一个实际的用户（MSI路由）将会随后启用这些映射。

  > 这部分涉及到kvmtool的中断虚拟化框架，单独列一节分析，见 `3.1 c)` 。

## 1.3 qemu虚机内存配置







## 1.3 kvm配置虚拟机内存

先看一些基本数据结构，包括：`kvm_userspace_memory_region`、`kvm_memory_slot`、`kvm_memslots`：

```c
/* for KVM_SET_USER_MEMORY_REGION */
struct kvm_userspace_memory_region {
	__u32 slot;
	__u32 flags;
	__u64 guest_phys_addr; 			//gpa
	__u64 memory_size; /* bytes */	//ram_size
	__u64 userspace_addr; /* start of the userspace allocated memory */ //hva
};

struct kvm_memslots {                                     
u64 generation;
short id_to_index[KVM_MEM_SLOTS_NUM];
atomic_t lru_slot;
int used_slots;                                        
struct kvm_memory_slot memslots[];                     
};

struct kvm_memory_slot {
	gfn_t base_gfn;                                       
	unsigned long npages;                                  
	unsigned long *dirty_bitmap;
	struct kvm_arch_memory_slot arch;
	unsigned long userspace_addr;                          
	u32 flags;
	short id;                                            
	u16 as_id;                                           
};
```

kvm一共为虚拟机定义了如下图所示的两级slot数组结构：

![img](https://pic1.zhimg.com/v2-05c2c810bd9bbf99d2cc7e6707a592e8_b.jpg)

其中第一级数组默认只有一个元素，因此每次内存设置操作都会向第二级数组中添加一个内存slot，用于表示虚拟机的内存条。

> KVM 还提供了 `KVM_SET_USER_MEMORY_REGION2` 来扩展原有的内存设置，需要单独分析，具体见kvm-api.rst和[guest_memfd](https://lwn.net/Articles/949123/)。

---

KVM的虚拟机内存设置操作就是将用户态传入的内存配置信息，转换为kvm中的guest内存描述结构体 `struct kvm_memory_slot`，并添加到memslots数组中。其代码流程如下：

```c
kvm_vm_ioctl_set_memory_region
    +-> kvm_set_memory_region
    	+-> __kvm_set_memory_region
    		+-> kvm_set_memslot
    			+-> //
```

[【原创】Linux虚拟化KVM-Qemu分析（五）之内存虚拟化 - LoyenWang - 博客园 (cnblogs.com)](https://www.cnblogs.com/LoyenWang/p/13943005.html)

[虚拟化 - 知乎 (zhihu.com)](https://www.zhihu.com/column/c_1520029500636696576)

[ARM64 KVM工作流程分析_kvm_handle_guest_abort-CSDN博客](https://blog.csdn.net/sungeshilaoda/article/details/89430820)







# 2 Guest MMIO退出处理框架

整体上qemu/kvmtool/kvm对Guest MMIO的退出处理，用户/内核都会对其进行处理（设备模拟单独分析）：

* 内核态：kvm
  * 解析Guest MMIO指令，然后根据MMIO所属设备、操作类型等，选择在内核或用户空间处理；
  * 用户态处理完MMIO，在内核态的信息同步；
* 用户态：qemu/kvmtool
  * 用户态根据 `kvm_run` 记录的信息处理MMIO；



# 3 设备模拟

## 3.1 kvmtool

### a) plic模拟



### b) serial8250模拟

- [x] [从kvmtools学习虚拟化四 IO端口 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/583203148)





### c) pci设备模拟

- [ ] [从kvmtools学习虚拟化五 PCI设备虚拟化 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/583203531)



### d) virtio设备模拟

- [ ] [从kvmtools学习虚拟化六 Virtio设备基础 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/583204723)
- [ ] [从kvmtools学习虚拟化七 Virtio Console的实现 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/583205729)





### c) kvmtool中断虚拟化框架

- [ ] [从kvmtools学习虚拟化七 Virtio Console的实现 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/583205729)
  - [ ] 中断管理
  - [ ] 中断路由
  - [ ] 中断注入





## 3.2 qemu





